NLP RESEARCH TASK

When machines process text, they cannot "understand" words directly — instead, words (symbols) are converted into numerical representations. This process is called feature extraction or vectorization. Different techniques have been developed, from simple frequency-based methods to advanced neural embeddings.


1. One-Hot Encoding

One-hot encoding is the simplest technique for converting words into numerical form so that machines can process them. It creates a vector representation for each word, where only one position in the vector is marked as 1 (the "hot" position), and all other positions are 0.

Example

Let’s say we have the following small corpus:

Sentence 1: "I love dogs"

Sentence 2: "I love cats"

Vocabulary: {I, love, dogs, cats} → size V = 4

One-hot vectors:

I → [1, 0, 0, 0]

love → [0, 1, 0, 0]

dogs → [0, 0, 1, 0]

cats → [0, 0, 0, 1]


Advantages :

Very simple and easy to implement.

Works well for small vocabularies.

Useful as a baseline for text representation.

Disadvantages :

Inefficient for large vocabularies (memory-intensive).

Ignores context (same representation for "bank" in river bank and money bank).

No semantic meaning: "dog" and "cat" are represented as completely different vectors with no similarity, even though they are related words.

In short, One-Hot Encoding is like giving each word a unique "ID badge," but without saying anything about what the word means or how it relates to other words.

2. Bag of Words (BoW)

It represents a document as a collection (or “bag”) of the words it contains, ignoring grammar and word order, but keeping track of how many times each word appears.


Explanation:

Represents each document as a vector of word frequencies.

Ignores grammar and order of words, only counts how often each appears.

Example:

Corpus:

D1: "The cat sat on the mat"

D2: "The dog chased the cat"

Step 1 – Vocabulary: {the, cat, sat, on, mat, dog, chased} → size = 7

Step 2 – Vectors:

D1 → [2, 1, 1, 1, 1, 0, 0]

D2 → [2, 1, 0, 0, 0, 1, 1]

(Index order: the, cat, sat, on, mat, dog, chased)

Advantages:

Simple and easy to implement → Beginner-friendly for NLP tasks.

Captures word frequency → Shows how often a word appears in a document.

Effective for small datasets → Useful in spam detection, sentiment analysis, and text classification.

Disadvantages:

Ignores word order and grammar → "The cat sat on the mat" = "On the mat sat the cat".

Inefficient with large vocabularies.

No semantic meaning captured → "cat" and "dog" are treated as unrelated as "cat" and "chased".

The point of BoW is to create a numerical representation of text based on word frequencies, making text analyzable by algorithms, even though it ignores grammar and meaning.


3. Term Frequency – Inverse Document Frequency (TF-IDF) 

TF-IDF is a numerical representation of text that measures how important a word is in a document relative to the entire collection (corpus).

It is built on top of Bag of Words, but instead of using plain counts, it weights words so that:

* Common words (like the, is, and) get lower scores.
* Rare but important words (like machine, neural, cancer) get higher scores.

TF -> FREQUENCY
IDF -> RARENESS
---

Formula

1. Term Frequency (TF):

   * How often a word appears in a document.
   * TF(t, d) = (Number of times term t appears in document d) / (Total words in document d)

2. Inverse Document Frequency (IDF):

   * How unique a word is across all documents.
   * IDF(t) = log(N / (1 + df(t)))
   * Where:

     * N = total number of documents
     * df(t) = number of documents containing term t

3. TF-IDF Score:

   * TF-IDF(t, d) = TF(t, d) × IDF(t)

---

Example

Corpus (2 documents):

* D1: "The cat sat on the mat"
* D2: "The dog chased the cat"

Step 1 – Vocabulary: {the, cat, sat, on, mat, dog, chased}

Step 2 – Term Frequency (TF):

* In D1: TF(cat) = 1/6, TF(the) = 2/6, TF(sat) = 1/6, etc.
* In D2: TF(dog) = 1/6, TF(the) = 2/6, TF(chased) = 1/6, etc.

Step 3 – Document Frequency (df):

* the → appears in both documents → df = 2
* cat → appears in both documents → df = 2
* dog → appears in D2 only → df = 1
* chased → appears in D2 only → df = 1
* sat, on, mat → appear in D1 only → df = 1

Step 4 – IDF:

According to smoothened version
* IDF(the) = log(2/2) = 0 → very low (common word) lower the idf score, common the word
* IDF(cat) = log(2/2) = 0 → very low
* IDF(dog) = log(2/1) = log(2) ≈ 0.30
* IDF(chased) = log(2/1) ≈ 0.30
* IDF(sat/on/mat) = log(2/1) ≈ 0.30

Step 5 – TF-IDF score:

* Words like the and cat get very low weight (0) → they’re common across documents.
* Words like dog, chased, sat, mat, on get higher weights → they’re more unique.

Thus, TF-IDF highlights important/rare words and downplays common words.

---

Advantages:

* Improves over BoW → Doesn’t just count, but weights words by importance.
* Reduces noise → Common words like "the" or "is" don’t dominate.
* Simple and effective → Still easy to compute, widely used in search engines, document ranking, keyword extraction.

Disadvantages:

* Still ignores word order & semantics → "good" and "bad" treated as unrelated.
* Sparse, high-dimensional vectors → Large vocabularies remain inefficient.
* Static weighting → Weights are based only on frequency, not meaning or context.

---

Real-World Use Cases:

* Search Engines (Google, Bing, etc.) → Rank documents based on keyword relevance.
* Text Classification → Spam detection, topic categorization.
* Keyword Extraction → Identify important terms in articles, research papers.

---

In short:
TF-IDF is like BoW with intelligence — it downplays common words and highlights rare but important ones, making it much more useful for information retrieval and classification.

---




   

