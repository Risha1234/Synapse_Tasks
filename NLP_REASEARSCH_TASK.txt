NLP RESEARCH TASK

When machines process text, they cannot "understand" words directly â€” instead, words (symbols) are converted into numerical representations. This process is called feature extraction or vectorization. Different techniques have been developed, from simple frequency-based methods to advanced neural embeddings.


1. One-Hot Encoding

One-hot encoding is the simplest technique for converting words into numerical form so that machines can process them. It creates a vector representation for each word, where only one position in the vector is marked as 1 (the "hot" position), and all other positions are 0.

Example

Letâ€™s say we have the following small corpus:

Sentence 1: "I love dogs"

Sentence 2: "I love cats"

Vocabulary: {I, love, dogs, cats} â†’ size V = 4

One-hot vectors:

I â†’ [1, 0, 0, 0]

love â†’ [0, 1, 0, 0]

dogs â†’ [0, 0, 1, 0]

cats â†’ [0, 0, 0, 1]


Advantages :

Very simple and easy to implement.

Works well for small vocabularies.

Useful as a baseline for text representation.

Disadvantages :

Inefficient for large vocabularies (memory-intensive).

Ignores context (same representation for "bank" in river bank and money bank).

No semantic meaning: "dog" and "cat" are represented as completely different vectors with no similarity, even though they are related words.

In short, One-Hot Encoding is like giving each word a unique "ID badge," but without saying anything about what the word means or how it relates to other words.

2. Bag of Words (BoW)

The Bag of Words model is one of the earliest and most widely used techniques in Natural Language Processing (NLP) to convert text into numerical form.

It represents a document as a collection (or â€œbagâ€) of the words it contains, ignoring grammar and word order, but keeping track of how many times each word appears.


Explanation:

Represents each document as a vector of word frequencies.

Ignores grammar and order of words, only counts how often each appears.

Example:

Corpus:

D1: "The cat sat on the mat"

D2: "The dog chased the cat"

Step 1 â€“ Vocabulary: {the, cat, sat, on, mat, dog, chased} â†’ size = 7

Step 2 â€“ Vectors:

D1 â†’ [2, 1, 1, 1, 1, 0, 0]

D2 â†’ [2, 1, 0, 0, 0, 1, 1]

(Index order: the, cat, sat, on, mat, dog, chased)

Advantages:

Simple and easy to implement â†’ Beginner-friendly for NLP tasks.

Captures word frequency â†’ Shows how often a word appears in a document.

Effective for small datasets â†’ Useful in spam detection, sentiment analysis, and text classification.

Disadvantages:

Ignores word order and grammar â†’ "The cat sat on the mat" = "On the mat sat the cat".

High-dimensional and sparse vectors â†’ Inefficient with large vocabularies.

No semantic meaning captured â†’ "cat" and "dog" are treated as unrelated as "cat" and "chased".

The point of BoW is to create a numerical representation of text based on word frequencies, making text analyzable by algorithms, even though it ignores grammar and meaning.


3. Term Frequency â€“ Inverse Document Frequency (TF-IDF)

TF-IDF is a numerical representation of text that measures how important a word is in a document relative to the entire collection (corpus).

It is built on top of Bag of Words, but instead of using plain counts, it weights words so that:

* Common words (like the, is, and) get lower scores.
* Rare but important words (like machine, neural, cancer) get higher scores.

---

## Formula

1. Term Frequency (TF):

   * How often a word appears in a document.
   * TF(t, d) = (Number of times term t appears in document d) / (Total words in document d)

2. Inverse Document Frequency (IDF):

   * How unique a word is across all documents.
   * IDF(t) = log(N / (1 + df(t)))
   * Where:

     * N = total number of documents
     * df(t) = number of documents containing term t

3. TF-IDF Score:

   * TF-IDF(t, d) = TF(t, d) Ã— IDF(t)

---

Example

Corpus (2 documents):

* D1: "The cat sat on the mat"
* D2: "The dog chased the cat"

Step 1 â€“ Vocabulary: {the, cat, sat, on, mat, dog, chased}

Step 2 â€“ Term Frequency (TF):

* In D1: TF(cat) = 1/6, TF(the) = 2/6, TF(sat) = 1/6, etc.
* In D2: TF(dog) = 1/6, TF(the) = 2/6, TF(chased) = 1/6, etc.

Step 3 â€“ Document Frequency (df):

* the â†’ appears in both documents â†’ df = 2
* cat â†’ appears in both documents â†’ df = 2
* dog â†’ appears in D2 only â†’ df = 1
* chased â†’ appears in D2 only â†’ df = 1
* sat, on, mat â†’ appear in D1 only â†’ df = 1

Step 4 â€“ IDF:

* IDF(the) = log(2/2) = 0 â†’ very low (common word)
* IDF(cat) = log(2/2) = 0 â†’ very low
* IDF(dog) = log(2/1) = log(2) â‰ˆ 0.30
* IDF(chased) = log(2/1) â‰ˆ 0.30
* IDF(sat/on/mat) = log(2/1) â‰ˆ 0.30

Step 5 â€“ TF-IDF score:

* Words like the and cat get very low weight (0) â†’ theyâ€™re common across documents.
* Words like dog, chased, sat, mat, on get higher weights â†’ theyâ€™re more unique.

Thus, TF-IDF highlights important/rare words and downplays common words.

---

Advantages:

* Improves over BoW â†’ Doesnâ€™t just count, but weights words by importance.
* Reduces noise â†’ Common words like "the" or "is" donâ€™t dominate.
* Simple and effective â†’ Still easy to compute, widely used in search engines, document ranking, keyword extraction.

Disadvantages:

* Still ignores word order & semantics â†’ "good" and "bad" treated as unrelated.
* Sparse, high-dimensional vectors â†’ Large vocabularies remain inefficient.
* Static weighting â†’ Weights are based only on frequency, not meaning or context.

---

Real-World Use Cases:

* Search Engines (Google, Bing, etc.) â†’ Rank documents based on keyword relevance.
* Text Classification â†’ Spam detection, topic categorization.
* Keyword Extraction â†’ Identify important terms in articles, research papers.

---

In short:
TF-IDF is like BoW with intelligence â€” it downplays common words and highlights rare but important ones, making it much more useful for information retrieval and classification.

---


4. Word Embeddings

Computers normally treat words as separate symbols. For example:
â€œcatâ€ â†’ 1, â€œdogâ€ â†’ 2, â€œappleâ€ â†’ 3
This approach does not capture meaning. The computer cannot tell that cat and dog are related, but cat and apple are not.

Word embeddings fix this by representing words as vectors of real numbers in a continuous space. Words with similar meanings are close to each other in this space.

Example:
king â‰ˆ [0.21, -0.34, 0.56, â€¦]
queen â‰ˆ [0.19, -0.40, 0.60, â€¦]
apple â‰ˆ [-0.90, 0.12, 0.45, â€¦]

In this space:
king â€“ man + woman â‰ˆ queen
cat will be close to dog, but far from apple.

   Word2Vec

Word2Vec is a popular algorithm created by Google in 2013 to generate word embeddings. It has two main architectures:

1. Continuous Bag of Words (CBOW)
	â€¢	Predicts a word from its context (neighboring words).
	â€¢	Example: sentence = â€œThe cat sits on the matâ€
Input context = {â€œTheâ€, â€œsitsâ€, â€œonâ€, â€œtheâ€}
Model predicts = â€œcatâ€

CBOW is fast and works well with smaller datasets.

2. Skip-Gram
	â€¢	Does the opposite: predicts context words from a given word.
	â€¢	Example: input word = â€œcatâ€
Model predicts neighbors like {â€œTheâ€, â€œsitsâ€, â€œonâ€}

Skip-gram works better with large datasets and captures rare words more effectively.

â¸»

How It Works
	1.	The model is a simple neural network with:
	â€¢	input layer (one-hot encoded word)
	â€¢	hidden layer (the embedding layer)
	â€¢	output layer (probabilities of context words)
	2.	During training, the network learns to predict words in context.
	3.	After training, the hidden layer weights become the word embeddings.

â¸»

Why Word2Vec is Useful
	â€¢	Captures semantic similarity (Paris is close to London, far from banana).
	â€¢	Supports analogy reasoning (king â€“ man + woman â‰ˆ queen).
	â€¢	More efficient than older methods like TF-IDF or count-based models.
	â€¢	Pretrained models are available such as Google News Word2Vec, GloVe, and fastText.

â¸»

Limitations of Word2Vec
	â€¢	Each word has only one vector, so multiple meanings of the same word cannot be distinguished.
	â€¢	Does not capture sentence-level meaning.
	â€¢	Needs large amounts of text data for high quality embeddings.
	â€¢	Rare words or unseen words are hard to handle.
	â€¢	Context is ignored, so apple in â€œapple pieâ€ and â€œApple Inc.â€ will have the same vector.
	â€¢	Does not handle subword information like prefixes or suffixes.
	â€¢	Has been overtaken by contextual models such as BERT and GPT, which create embeddings that change with context.

   GloVe
   
Step 1: Word co-occurrence matrix

Imagine you scan a huge text corpus and count:
	â€¢	How often does word i appear near word j?
This gives you a big matrix of wordâ€“word co-occurrence.
Example (simplified):
So this table encodes how often words co-occur across the whole corpus.

â¸»

Step 2: Probability of co-occurrence

From the matrix, we compute probabilities like:

P(j | i) = probability that word j appears in the context of word i

For example:
P(deep | like) = count(like, deep) / total co-occurrences of â€œlikeâ€

â¸»

Step 3: Ratio of probabilities

The key trick in GloVe is: ratios of probabilities reveal meaning.

Example with the words: ice, steam, solid, gas.
	â€¢	P(solid | ice) is high (because ice often appears near solid).
	â€¢	P(solid | steam) is low.
	â€¢	P(gas | steam) is high.
	â€¢	P(gas | ice) is low.

So the ratio P(solid | ice) / P(solid | steam) is very large, while
P(gas | ice) / P(gas | steam) is very small.

These ratios capture meaningful differences in how words are used.

â¸»

Step 4: Objective function

GloVe trains word vectors so that:

dot product(word_i, word_j) â‰ˆ log(co-occurrence(i, j))

In other words: if two words co-occur often, their vectors should have a large dot product.

â¸»

Step 5: Training
	â€¢	Start with random word vectors.
	â€¢	Use gradient descent to minimize the difference between predicted co-occurrence and actual co-occurrence.
	â€¢	Add a weighting function so very frequent words (like â€œtheâ€, â€œandâ€) donâ€™t dominate.

â¸»

Step 6: Result

When training finishes, we have dense word embeddings (say 50â€“300 dimensions).
Now:
	â€¢	Similar words have similar vectors (king, queen, prince).
	â€¢	Analogies work: king â€“ man + woman â‰ˆ queen.
	â€¢	You can use these vectors in downstream NLP tasks.
5. Contextual Word Embeddings

The Problem with Word2Vec and GloVe
	â€¢	Word2Vec/GloVe give static embeddings.
	â€¢	Example: the word â€œbankâ€ gets one vector no matter the sentence:
	â€¢	â€œI sat by the bank of the riverâ€
	â€¢	â€œI deposited money in the bankâ€
Both meanings are different, but the vector is the same.

â¸»

Contextual Embeddings (the fix)

Instead of one fixed vector per word, these models generate different embeddings depending on the context of the word in the sentence.

So:
	â€¢	â€œbankâ€ in sentence about rivers â†’ embedding near â€œwaterâ€, â€œshoreâ€.
	â€¢	â€œbankâ€ in sentence about money â†’ embedding near â€œfinanceâ€, â€œloanâ€.

This is why theyâ€™re called contextual word embeddings.

â¸»

ELMo (2018, by AllenNLP)
	â€¢	Name: Embeddings from Language Models.
	â€¢	Idea: Use a deep bidirectional LSTM language model.
	â€¢	How it works:
	1.	Train a language model on a huge corpus (predict the next word, given history).
	2.	Use hidden states from multiple LSTM layers as the embedding for each word.
	3.	Because itâ€™s bidirectional, ELMo looks at the whole sentence (both left and right context).

Example:
Sentence = â€œHe went to the bank to fish.â€
ELMo embedding for â€œbankâ€ will encode â€œfishâ€ as context, so the vector means â€œriverbankâ€.

â¸»

BERT (2018, by Google)
	â€¢	Name: Bidirectional Encoder Representations from Transformers.
	â€¢	Uses the Transformer architecture instead of LSTMs.
	â€¢	Key innovations:
	1.	Masked Language Model: Randomly mask some words and train the model to predict them.
Example: â€œThe cat sat on the [MASK]â€ â†’ model predicts â€œmatâ€.
	2.	Bidirectional Attention: Uses self-attention to look at the entire sentence (left and right) at once.
	â€¢	Result: For each token in the sentence, BERT produces a context-aware embedding.

Example:
Sentence = â€œI went to the bank to withdraw money.â€
	â€¢	BERT embedding for â€œbankâ€ here is near finance words.
Sentence = â€œThe fish swam near the bank.â€
	â€¢	BERT embedding for â€œbankâ€ here is near nature words.

â¸»

Comparing ELMo and BERT
	â€¢	ELMo uses LSTMs; BERT uses Transformers (much more powerful).
	â€¢	ELMo embeddings are contextual but not pre-trained as richly as BERT.
	â€¢	BERT pre-training + fine-tuning became the standard approach for NLP.

â¸»

Why Contextual Embeddings Matter
	â€¢	They solve polysemy (multiple meanings).
	â€¢	They improve performance on almost every NLP task (sentiment analysis, QA, translation, etc.).
	â€¢	They paved the way for large models like GPT.

6. Sentence & Document Embeddings

1. Doc2Vec (Paragraph Vector)
	â€¢	Extension of Word2Vec (by the same team at Google).
	â€¢	Idea: Just like Word2Vec learns word vectors, Doc2Vec learns an extra vector for the whole document (or sentence).
	â€¢	During training, the model predicts a word given its context + the document ID.
	â€¢	That â€œdocument ID vectorâ€ becomes the document embedding.

ğŸ‘‰ Example:
Sentence: â€œThe dog chased the cat.â€
	â€¢	Word2Vec gives vectors for dog, chased, cat.
	â€¢	Doc2Vec also learns a special â€œdocument vectorâ€ summarizing the whole sentence.

So even if the sentence is long, its embedding captures the gist.

â¸»

2. Sentence-BERT (SBERT)
	â€¢	Built on top of BERT.
	â€¢	Problem: BERT isnâ€™t designed to directly give good sentence embeddings (cosine similarity between two sentence embeddings isnâ€™t reliable).
	â€¢	SBERT solves this by adding a pooling layer on top of BERT outputs (like mean pooling or CLS token) and training with Siamese networks + similarity loss.

ğŸ‘‰ Example:
Sentences:
	1.	â€œA man is playing guitar.â€
	2.	â€œSomeone is making music with a guitar.â€

	â€¢	SBERT maps both to very close embeddings in vector space.
	â€¢	If you compute cosine similarity, you get a high score (~0.85+).

â¸»

3. Universal Sentence Encoder (USE)
	â€¢	From Google.
	â€¢	Aimed at being plug-and-play for sentence embeddings.
	â€¢	Built with Transformer or Deep Averaging Network (DAN) architectures.
	â€¢	Trained on a wide variety of tasks: conversation data, QA, web news, etc.
	â€¢	Output: a 512-dimensional fixed-size embedding for any sentence.

ğŸ‘‰ Example:
Sentence: â€œThe weather is nice today.â€
	â€¢	USE â†’ a single 512D vector (regardless of sentence length).
	â€¢	Good for downstream tasks like clustering, semantic search, sentiment classification.

â¸»

ğŸ”‘ Main differences:
	â€¢	Doc2Vec â†’ classical, simple, but weaker for semantic similarity.
	â€¢	Sentence-BERT â†’ state-of-the-art for semantic similarity & retrieval tasks.
	â€¢	USE â†’ easy-to-use, Googleâ€™s â€œuniversalâ€ approach, but less accurate than SBERT for fine-grained similarity.